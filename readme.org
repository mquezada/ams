* 7 nov 2016

- Pasos a seguir:
  - Elegir baselines
  - Escribir motivación del problema
  - Implementar baselines
  - Decidir ground truth(s)
  - Decidir forma(s) de evaluar
  - Evaluar!

- Indicar en la motivación que el trabajo relacionado no cumple todos nuestros requisitos.
- Por qué algunas técnicas no son suficientes para resolver el problema.

* 10 nov 2016

- Clasificar los papers por otros criterios:
  - Mencionan (y cuales) criterios de calidad de resumen
  - Consideran aspecto multimedia (no solo texto)
- Leer paper de Teresa para ver evaluacion


* 17 nov 2016
- Cuales son los mejores atributos que permiten generar resumenes
- Combinar los mejores features de alguna forma (aprenderlos) para generar los resumenes
- Baselines:
  - Random
  - Frequency
  - Learning to rank: estos son los datos, estos son los resumenes (gold-standard): “aprende de los datos”
- Implementar los baselines y diseñar el experimento
- … con eso diseñar la solucion propuesta y experimentar “en grande”
- Hablar con la Vanessa sobre el experimento para separar los atributos y medir sus aportes individuales?

* 18 nov 2016
- TODOs Baselines:
  - Implementar preprocesamiento de texto
  - Tratar con la no-redundancia
  - Considerar documentos en vez de tweets?
  - Leer e implementar? La metodologia de TweetMotif
  - Mirar los baselines que sean mas directos de implementar y que calcen muy bien con nuestros objetivos:
  - Hacer resumenes de (seleccionar) tweets O de documentos que son apuntados por los tweets

* 29 dic 2016
** Ideas para continuar
   1. aumentar tweets con palabras del word2vec, y usar tf-idf para modelar tweets/documentos
   2. representar tweets/documentos como promedio de vectores de las palabras en w2v
   3. lo mismo que 2, más aumentar tweets con palabras del w2v
   4. usar doc2vec para representar tweets/docs
   5. hacer grafo del evento usando w2v del evento + POS
   6. lo mismo que 5, usando w2v del corpus
* 5 ene 2017
** Semana anterior
   - doc2vec o word2vec en el mismo evento no funciona bien (pocos datos)
   - mgraph funcionando!
** Eliminar ruido de eventos
*** Experimento
   1. agrupar tweets por URL (= documento)
      - tweets sin URL son el documento correspondiente al mismo tweet
   2. aumentar documentos usando palabras mas similares en w2v general
   3. aplicar tf-idf
   4. hacer clustering (k-means)
*** Evaluacion
    - propiedades estructurales del clustering
      - SSE
      - diametro
      - silhouette
      - distancia intercluster
        - min
        - max
        - avg (centroide)
*** Parametros
    - Umbral de similitud W2V
      - umbral fijo
      - umbral fijo + codo (2da derivada)
    - numero de clusters en kmeans
    - distancia euclidiana o disimilitud coseno
    - eliminar tweets duplicados
    
* 6 ene 2017
  - mirar localized pagerank?
  - elegir un evento, y graficar en 2D las palabras + palabras mas cercanas usando modelo completo
   
* 17 ene 2017

- find fake news?
- classify events by topic using WE analogies
- removing noise (spam) from tweets
- MMR

- muc (message undestanding conf), tdt, trec 
- explainability of neural networks
- analogous events
- generating summaries
- finding sub-topics
- data cleaning/curation/detection
- applying structure to a bag of tweets
- user weight on credibility

* 20 mar 2017
  - instaladas librerias para jupyter:
    - qgrid: filtros en pandas df
      - https://github.com/quantopian/qgrid
    - Compact Language Detector 2: identificar idiomas
      - https://github.com/CLD2Owners/cld2
    - watermark: info de hardware en jupyter
      - https://github.com/rasbt/watermark/
    - probablepeople: resolver nombres de personas
      - https://github.com/datamade/probablepeople
        
  - =jupyter console --existing= abre un ipython que comparte las variables del notebook abierto
  - =%load_ext autoreload; %autoreload 2= actualiza los modulos (no lo he probado)
* 24 mar 2017
- Diseñar la medida de distancia
- Implementar o buscar un clustering que use esta medida
- Diseñar interfaz para ver los resultados

* 31 mar 2017
- LM: charla de tesis I

* 18 abr 2017
- MQ: presentación de examen de propuesta

* 21 abr 2017
- definir tipo de evaluacion que podemos usar
  - hay alguno que podemos usar de los papers vistos?
  - MGraph
  - paper Teresa de evaluacion (su ultimo paper ICM) de outputs de algoritmos
- hay algun ground truth?
  - Mirar TREC summarization
- mirar paper de mor naaman
- escribir modelo

* 24 abr 2017
** Avance
- definicion modelo: https://github.com/mquezada/ams/blob/clustering/tex/model.pdf

* 28 abr 2017
** Avance
- Se agregaron referencias de Social Anchor Text y Multimodal summarization: https://docs.google.com/spreadsheets/d/1TZadD1nYuPd6N6qogtkGdmoqBhWQy4RF2wkoVCt2qKc/edit#gid=175296704

* 5 may 2017
** Avance
- es difícil inspeccionar los clusters debido a la alta redundancia de los datos (muchos tweets repetidos)
- se probó con TwitterLDA y clustering jerárquico
- se quedó en usar sólo un representante por documento

* 9 may 2017
** Avance
- documentos ahora estan representados por 1 tweet:
  - si varios tweets retuitean a un tweet, éste último es el representante
  - si varios tweets comparten una url u, cualquiera de éstos es el representante
- Los resultados de hacer clustering con kmeans (mini batch kmeans) mejoran con respecto a la representacion anterior (1 documento = todos los tweets, o los tweets individuales), sin embargo, clusters contienen overlap de topicos al mirar evento libya_hotel
- Al usar CLUTO con clustering aglomerativo, los resultados se ven mucho mejor un par de eventos inspeccionados (oscar_pistorius y libya_hotel). Con un evento no es fácil decir cómo son los clusters (microsoft_nokia).

** Queda por definir
1. determinar número de clusters apropiados (fijo o variable)
2. elegir representantes por cluster
3. aplicaciones o formas de evaluar el clustering / resumen

** Trabajo para la próxima reunión
- Fijar número de clusters de forma manual y probar distintas formas de clustering
  - idea: elegir el clustering con un numero fijo de clusters para facilitar la eleccion del clustering. Despues nos preocupamos de (1)
- Elegir representantes por cluster de manera naive / simple
  - ejemplo: numero de tweets asociados al cluster, numero de rts, favs, replies
- Pensar en (3)
  - ejemplo: comparar con pagina en wikipedia, o considerar otro ground truth (reportaje en cnn, bbc, etc)
  - al mirar wikipedia, comparar el overlap de topicos de wikipedia vs nuestro resumen y ver si es posible sugerir edits 
  - metadata de eventos para exploracion (galean?)
* 18 may 2017

** Cosas pendientes:
   - Metodología para elegir nº clusters:
     - Fijo (más proceso para eliminar redundantes)
     - Variable, dependiendo de cada evento
   - Criterio para elegir representantes de clusters (o una mezcla de estos):
     - RTs
     - Favs
     - total tweets
     - total replies
     - contenido
     - authority
     - URL info (domain, etc)
     
   - Elegir el metodo de clustering:
     - TwitterLDA
     - Aglomerativo
     - KMeans
     - Incremental
     - Grafo

** Plan
   - Fijar numero de clusters para distintos eventos
   - elegir representantes usando criterios simples

** Topicos por evento
*** Oscar Pistorius Trial
    1. Apology (quizas 2)
    2. Brother car crash
    3. Mental evaluation / Psychiatric test
    4. Paddi power
    5. Vomit in court
    6. Baba security guard everything is fine
    7. Stand in court without prosthetic legs
    8. Prosecutor cross examination (quizas 2 o 3)
    9. Friend take blame restaurant shooting
    10. Text to Reeva days before

*** Mumbai Rape
    1. Incidente mismo
    2. Algo relacionado con la victima
    3. Atrapan al 4º culpable
    4. Atrapan 5º culpable

*** Libya Hotel attack
    1. Auto bomba
    2. Claim ISIS
    3. Storm bullets?
    4. Muertes de extranjeros y guardias
    5. Spam (x3)

*** Nepal earthquake
    1. Everest avalanch
    2. Muertos (x2)
    3. Kathmandu (Dharahara) building collapses
    4. Help/Aid
    5. Million dollar rescue team
    6. Reporte del terremoto mismo
* 19 may 2017
** Metodología de generación de summaries:
   - Doc model
   - Método de clustering:
     - no. de clusters
     - algoritmo
     - medida de similitud
   - Elección de representantes
     - Cómo?
     - Cuántos?

** Evaluación
   - Evaluación manual: se pueden probar pocas opciones
   - Evalaución (semi-)automática: se pueden probar todas las opciones
     - generación manual de ground truth + evaluación automática

** Puntos pendientes
   - Metodología de resúmenes
   - Eliminar clusters/docs de spam o ruido
   - Buscar medidas automáticas de evaluación: ROUGE, NMI?, etc.
   - Medir precisión/recall de tópicos entre resúmenes y groundtruth
   - Generar ground truths (wikipedia (colectivo), medios de noticias (expertos))
   - Grid search entre grountruth y metodologías para encontrar parámetros óptimos
